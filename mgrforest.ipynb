{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "import numpy as np\n",
    "# from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cv2 as cv\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('/home/qian/桌面/413新作数据集/npy/X_17296_new.npy')\n",
    "y = np.load('/home/qian/桌面/413新作数据集/npy/y_17296_new.npy')\n",
    "# X = X.astype('float32') / 255.\n",
    "aa=15000\n",
    "X_train = X[:aa]\n",
    "y_train = y[:aa]\n",
    "X_test =  X[aa:]\n",
    "y_test =  y[aa:]\n",
    "X = 0\n",
    "y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = '0008_MSCF_17296.png'\n",
    "fenge4 = np.load('/home/qian/桌面/413新作数据集/对比/0008_4510_564.npy')\n",
    "row_side,loc_side =510,564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Created on Tue Mar 19 14:26:39 2019\n",
    "\n",
    "@author: songwenzhu\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\n",
    "# from sklearn.cross_validation import cross_val_predict as cvp\n",
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "import random\n",
    "from functools import reduce\n",
    "\n",
    "class MultiGrainedScaner():\n",
    "    def __init__(self, base_estimator, params_list, sliding_ratio = 0.25, k_fold = 3):\n",
    "        if k_fold > 1: #use cv\n",
    "            self.params_list = params_list\n",
    "        else:#use oob\n",
    "            self.params_list = [params.update({'oob_score':True}) or params for params in params_list]\n",
    "        self.sliding_ratio = sliding_ratio\n",
    "        self.k_fold = k_fold\n",
    "        self.base_estimator = base_estimator\n",
    "        klass = self.base_estimator.__class__\n",
    "        self.estimators = [klass(**params) for params in self.params_list]\n",
    "\n",
    "    #generate scaned samples, X is not None, X[0] is no more than 3d\n",
    "    def _sample_slicer(self,X,y):\n",
    "        data_shape = X[0].shape\n",
    "        stride = 7\n",
    "        window_shape = [max(int(data_size * self.sliding_ratio),1) for data_size in data_shape]\n",
    "        scan_round_axis = [int((data_shape[i]-window_shape[i])/stride+1) for i in range(2)]\n",
    "        scan_round_total = reduce(lambda acc,x: acc*x,scan_round_axis)\n",
    "        if len(data_shape) == 1:\n",
    "            newX = np.array([x[beg * window_shape[0]:(beg+1)*window_shape[0]]\n",
    "                                for x in X\n",
    "                                    for beg in range(scan_round_axis[0])])\n",
    "        elif len(data_shape) == 2: #ravel 拉伸\n",
    "            newX = np.array([x[beg0*stride:beg0*stride+window_shape[0],beg1*stride:beg1*stride+window_shape[1]].ravel()\n",
    "                                for x in X\n",
    "                                    for beg0 in range(scan_round_axis[0])\n",
    "                                        for beg1 in range(scan_round_axis[1])])\n",
    "        elif len(data_shape) == 3:\n",
    "            newX = np.array([x[beg0 * stride:beg0 * stride + window_shape[0],beg1 * stride:beg1*stride + window_shape[1]].ravel()\n",
    "                                for x in X\n",
    "                                    for beg0 in range(scan_round_axis[0])\n",
    "                                        for beg1 in range(scan_round_axis[1])])\n",
    "        newy = y.repeat(scan_round_total)\n",
    "        return newX,newy,scan_round_total\n",
    "\n",
    "    #generate new sample vectors\n",
    "    def scan_fit(self,X,y):\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        newX,newy,scan_round_total = self._sample_slicer(X,y)\n",
    "        sample_vector_list = []\n",
    "        for estimator in self.estimators:\n",
    "            estimator.fit(newX, newy)\n",
    "            if self.k_fold > 1:# use cv\n",
    "                predict_ = cvp(estimator, newX, newy, cv=self.k_fold, n_jobs = -1)\n",
    "            else:#use oob\n",
    "                predict_ = estimator.oob_decision_function_\n",
    "                #fill default value if meet nan\n",
    "                inds = np.where(np.isnan(predict_))\n",
    "                predict_[inds] = 1./self.n_classes\n",
    "            sample_vector = predict_.reshape((len(X),scan_round_total*self.n_classes))\n",
    "            sample_vector_list.append(sample_vector)\n",
    "        return np.hstack(sample_vector_list)\n",
    "\n",
    "    def scan_predict(self,X):\n",
    "        newX,newy,scan_round_total = self._sample_slicer(X,np.zeros(len(X)))\n",
    "        sample_vector_list = []\n",
    "        for estimator in self.estimators:\n",
    "            predict_ = estimator.predict(newX)\n",
    "            sample_vector = predict_.reshape((len(X),scan_round_total*self.n_classes))\n",
    "            sample_vector_list.append(sample_vector)\n",
    "        return np.hstack(sample_vector_list)\n",
    "    \n",
    "scan_forest_params1 = RandomForestClassifier(n_estimators=10,min_samples_split=21,max_features='sqrt',n_jobs=-1).get_params()\n",
    "scan_forest_params2 = ExtraTreesClassifier(n_estimators = 10,min_samples_split=21, n_jobs=-1).get_params()\n",
    "# 随机森林参数\n",
    "# criterion 不纯度的衡量指标 有基尼系数和信息熵两种选择\n",
    "# maxdepth \n",
    "# n_estimators=30 森林中树木的个数\n",
    "# max_features=20 限制分支时考虑的特征个数 超过限制个数的特征都会被舍弃 默认值为总特征个数开平方取整\n",
    "# cascade_forest_params1 = RandomForestClassifier(n_estimators=1000,min_samples_split=11,max_features='sqrt',n_jobs=-1).get_params()\n",
    "# cascade_forest_params2 = ExtraTreesClassifier(n_estimators = 1000,min_samples_split=11, n_jobs=-1).get_params()\n",
    "scan_params_list = [scan_forest_params1,scan_forest_params2]\n",
    "# cascade_params_list = [cascade_forest_params1,cascade_forest_params2]*2\n",
    "\n",
    "# def calc_accuracy(pre,y):\n",
    "#     return float(sum(pre==y))/len(y)\n",
    "class ProbRandomForestClassifier(RandomForestClassifier):\n",
    "    def predict(self, X):\n",
    "        return RandomForestClassifier.predict_proba(self, X)\n",
    "    \n",
    "\n",
    "\n",
    "class RFLayer_RAND(object):\n",
    "    def __init__(self, n_estimators, classifier=True , md=None, mss=10):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = md\n",
    "        self.min_samples_split = mss\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def fit(self, X_train, y_train, kfold=5, k=1, n_jobs=-1): # kfold = 5 yields 80/20 split, k will be the number of times we run validation\n",
    "        if kfold > 1:\n",
    "            kf = KFold(kfold, shuffle=True)\n",
    "        else:\n",
    "            raise ValueError('Need to pass kfold something greater than 1 so can do cross validation')\n",
    "\n",
    "        models = []\n",
    "        best_score = 0\n",
    "        best_ind = 0\n",
    "        count = 0\n",
    "\n",
    "        # split training data into training and estimating sets via quasi kfold validation routine\n",
    "        for tr_ind, est_ind in kf.split(X_train, y_train):\n",
    "            # instantiate the layer of decision trees\n",
    "            models.append(RandomForestClassifier(self.n_estimators, criterion='gini', max_depth=self.max_depth,\n",
    "                                                 min_samples_split=self.min_samples_split,min_samples_leaf = 1,\n",
    "                                                 max_features = 'sqrt',\n",
    "                                                 n_jobs=n_jobs))\n",
    "#             for tree in models[count].self.estimators_: # make half of the trees completely random Decision Trees\n",
    "#             for tree in models[count].:\n",
    "#                 if np.random.rand() <= .5:\n",
    "#                     tree.splitter = 'random'\n",
    "\n",
    "\n",
    "            # get the split of the training data\n",
    "            X_tr, y_tr = X_train[tr_ind,:], y_train[tr_ind]\n",
    "            # train the layer on this split\n",
    "            models[count].fit(X_tr, y_tr)\n",
    "            X_tr, y_tr = 0, 0\n",
    "\n",
    "            # check accuracy on the estimation set 测试集\n",
    "            X_est, y_est = X_train[est_ind,:], y_train[est_ind]\n",
    "            y_pred = models[count].predict(X_est)\n",
    "            acc_score = accuracy_score(y_pred, y_est)\n",
    "            X_est, y_est = 0, 0 # memory\n",
    "            y_pred = 0 # memory\n",
    "\n",
    "            if acc_score > best_score: # with k > 1 we compare to see which is best layer trained\n",
    "                best_score = acc_score\n",
    "                best_ind = count\n",
    "            count += 1\n",
    "            if count >= k:\n",
    "                break\n",
    "\n",
    "        # save the best layer\n",
    "        self.L = models[best_ind]\n",
    "        self.n_classes = self.L.n_classes_\n",
    "        self.val_score = best_score\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.L.predict(X_test)\n",
    "\n",
    "    def push_thru_data(self, X):\n",
    "        n_samples, dim_data = X.shape\n",
    "        X_push = np.empty((n_samples, self.n_estimators*self.n_classes))\n",
    "        # push the data X through this layer\n",
    "        i = 0\n",
    "        for tree in self.L.estimators_:\n",
    "            if self.classifier:\n",
    "                X_push[:,i*self.n_classes:(i+1)*self.n_classes] = tree.predict_proba(X)    \n",
    "            i += 1\n",
    "        X_a = np.concatenate((X_push,X[:,:n*self.n_classes]),axis = 1)\n",
    "        return X_a\n",
    "#         if X[1].shape == (n*5,):\n",
    "#             X_push = np.concatenate((X_push,X[:,n*5:]),axis = 1)\n",
    "#         elif X[1].shape == (n*10,):\n",
    "#             X_push = np.concatenate((X_push,X[:,n*5:]),axis = 1)\n",
    "#         else:\n",
    "#             X_push = X_push\n",
    "#             \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training samples scanning.....\n",
      " training samples: (15000, 250)\n",
      "RF Layer training:\n",
      "(15000, 250)\n",
      "Layer 1\n",
      "acc: 0.9388\n",
      "Going to another layer\n",
      "(15000, 2750)\n",
      "Layer 2\n",
      "acc: 0.962\n",
      "Going to another layer\n",
      "(15000, 5000)\n",
      "Layer 3\n",
      "acc: 0.9766\n",
      "Going to another layer\n",
      "(15000, 5000)\n",
      "Layer 4\n",
      "acc: 0.9838\n",
      "Converged! Stopping building layers\n",
      "training time: 137.39619207382202\n"
     ]
    }
   ],
   "source": [
    "# Multi-Grained Scan Step\n",
    "Scaner1 = MultiGrainedScaner(ProbRandomForestClassifier(), scan_params_list, sliding_ratio = 1./2)\n",
    "Scaner2 = MultiGrainedScaner(ProbRandomForestClassifier(), scan_params_list, sliding_ratio = 1./4)\n",
    "\n",
    "# Scaner3 = MultiGrainedScaner(ProbRandomForestClassifier(), scan_params_list, sliding_ratio = 1./16)\n",
    "\n",
    "print('start training samples scanning.....')\n",
    "import time\n",
    "st = time.time()\n",
    "X_train_scan =np.hstack([scaner.scan_fit(X_train, y_train)\n",
    "                             for scaner in [Scaner1,Scaner2]])\n",
    "print(' training samples:',X_train_scan.shape)\n",
    "\n",
    "# train the next layers on multigrained scanning data\n",
    "print( 'RF Layer training:')\n",
    "\n",
    "\n",
    "# parameters for the building of the next layers\n",
    "n = 500# num trees in each layer\n",
    "min_gain = 0.01\n",
    "verbose = True\n",
    "max_layers = 5\n",
    "md = None\n",
    "mss = 21\n",
    "n_jobs = -1\n",
    "\n",
    "# dictionary where layers of decision trees will be stored\n",
    "Layers = {}\n",
    "\n",
    "prev_score = -1.0 # instantiate prev_score\n",
    "# build the layers\n",
    "for i in range(max_layers):\n",
    "    print (X_train_scan.shape)\n",
    "    RFL = RFLayer_RAND(n, md=md, mss=mss)\n",
    "    RFL.fit(X_train_scan, y_train, 3, 1, n_jobs)\n",
    "    Layers[i] = RFL\n",
    "\n",
    "    # if verbose, print out the estimation accuracy for this layer\n",
    "    if verbose:\n",
    "        print ('Layer ' + str(i+1))\n",
    "        print ('acc: ' + str(RFL.val_score))\n",
    "\n",
    "\n",
    "    # check to see if we have improved enough going one more layer\n",
    "    rel_gain = (RFL.val_score - prev_score)/float(abs(prev_score))\n",
    "    if rel_gain < min_gain or RFL.val_score == 1.0 :\n",
    "        print ('Converged! Stopping building layers')\n",
    "        print\n",
    "        break\n",
    "    prev_score = RFL.val_score\n",
    "\n",
    "    # if moving on to another level, push the data through\n",
    "    X_train_scan = RFL.push_thru_data(X_train_scan)\n",
    "    print ('Going to another layer')\n",
    "    print\n",
    "    \n",
    "et = time.time()\n",
    "print('training time:',et - st)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "start training samples scanning.....\n",
    " training samples: (15000, 890)\n",
    "RF Layer training:\n",
    "(15000, 890)\n",
    "Layer 1\n",
    "acc: 0.9556\n",
    "Going to another layer\n",
    "(15000, 5890)\n",
    "Layer 2\n",
    "acc: 0.9658\n",
    "Going to another layer\n",
    "(15000, 10000)\n",
    "Layer 3\n",
    "acc: 0.982\n",
    "Going to another layer\n",
    "(15000, 10000)\n",
    "Layer 4\n",
    "acc: 0.9882\n",
    "Converged! Stopping building layers\n",
    "training time: 597.6242744922638\n",
    "Loading in testing data\n",
    "Load over\n",
    "training time: 39.19164228439331\n",
    "Statistics:\n",
    "The accuracy was:\n",
    "0.951219512195\n",
    "Params:\n",
    "num_tres in each layer = 1000\n",
    "md =None\n",
    "mss = 21/d\n",
    "\n",
    "\n",
    "start training samples scanning.....\n",
    " training samples: (15000, 890)\n",
    "RF Layer training:\n",
    "(15000, 890)\n",
    "Layer 1\n",
    "acc: 0.9494\n",
    "Going to another layer\n",
    "(15000, 10890)\n",
    "Layer 2\n",
    "acc: 0.9714\n",
    "Going to another layer\n",
    "(15000, 20000)\n",
    "Layer 3\n",
    "acc: 0.985\n",
    "Going to another layer\n",
    "(15000, 20000)\n",
    "Layer 4\n",
    "acc: 0.9894\n",
    "Converged! Stopping building layers\n",
    "training time: 1865.0062873363495\n",
    "Loading in testing data\n",
    "Load over\n",
    "training time: 138.57394194602966\n",
    "Statistics:\n",
    "The accuracy was:\n",
    "0.952961672474\n",
    "Params:\n",
    "num_tres in each layer = 2000\n",
    "md =Nonzhuany\n",
    "mss = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading in testing data\n",
      "Load over\n",
      "testing time: 11.196585893630981\n",
      "Statistics:\n",
      "The accuracy was:\n",
      "0.9425087108013938\n",
      "Params:\n",
      "num_tres in each layer = 500\n",
      "md =None\n",
      "mss = 21\n"
     ]
    }
   ],
   "source": [
    "# load in testing data, free up memory of the training data\n",
    "print ('Loading in testing data')\n",
    "# X_test = X_test(10000,784)\n",
    "# X_t_curr = X_t_curr.astype('uint8')\n",
    "# y_t = y_t.astype('uint8')\n",
    "import time\n",
    "st = time.time()\n",
    "X_a_scan = np.hstack([scaner.scan_predict(X_test)\n",
    "                             for scaner in [Scaner1,Scaner2]])\n",
    "print ('Load over')\n",
    "# push test data thru FTDRF layers\n",
    "for i in range(len(Layers.keys())-1):\n",
    "    X_a_scan = Layers[i].push_thru_data(X_a_scan)\n",
    "last = len(Layers.keys())-1\n",
    "y_pred = Layers[last].predict(X_a_scan)\n",
    "\n",
    "et = time.time()\n",
    "print('testing time:',et - st)\n",
    "\n",
    "print ('Statistics:')\n",
    "print ('The accuracy was:')\n",
    "print (accuracy_score(y_pred, y_test))\n",
    "print ('Params:')\n",
    "print ('num_tres in each layer = ' + str(n))\n",
    "print ('md =' + str(md))\n",
    "print ('mss = ' + str(mss))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cifar10 对比\n",
    "start training samples scanning.....\n",
    " training samples: (50000, 500)\n",
    "RF Layer training:\n",
    "(50000, 500)\n",
    "Layer 1\n",
    "acc: 0.464870702586\n",
    "Going to another layer\n",
    "(50000, 5500)\n",
    "Layer 2\n",
    "acc: 0.782264354713\n",
    "Going to another layer\n",
    "(50000, 10000)\n",
    "Layer 3\n",
    "acc: 0.915281694366\n",
    "Going to another layer\n",
    "(50000, 10000)\n",
    "Layer 4\n",
    "acc: 0.966340673187\n",
    "Going to another layer\n",
    "(50000, 10000)\n",
    "Layer 5\n",
    "acc: 0.983680326393\n",
    "Going to another layer\n",
    "training time: 2297.896358013153\n",
    "Loading in testing data\n",
    "Load over\n",
    "training time: 128.4914653301239\n",
    "Statistics:\n",
    "The accuracy was:\n",
    "0.94699\n",
    "Params:\n",
    "num_tres in each layer = 500\n",
    "md =None\n",
    "mss = 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = 0\n",
    "X_train_scan = 0\n",
    "y_train = 0 # memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fenge41 = fenge4[:100000]\n",
    "fenge41.shape\n",
    "fenge42 = fenge4[100000:200000]\n",
    "fenge43 = fenge4[200000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a_scan1 = np.hstack([scaner.scan_predict(fenge41.reshape((len(fenge41),28,28,4)))\n",
    "                             for scaner in [Scaner1,Scaner2]])\n",
    "X_a_scan2 = np.hstack([scaner.scan_predict(fenge42.reshape((len(fenge42),28,28,4)))\n",
    "                             for scaner in [Scaner1,Scaner2]])\n",
    "X_a_scan3 = np.hstack([scaner.scan_predict(fenge43.reshape((len(fenge43),28,28,4)))\n",
    "                             for scaner in [Scaner1,Scaner2]])\n",
    "# X_a_scan = np.concatenate((X_a_scan1,X_a_scan2,X_a_scan3),axis = 0)\n",
    "# X_a_scan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2296, 5000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_a_scan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成三分之一\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Layers.keys())-1):\n",
    "    X_a_scan1 = Layers[i].push_thru_data(X_a_scan1)\n",
    "last = len(Layers.keys())-1\n",
    "y_pred1 = Layers[last].predict(X_a_scan1)\n",
    "print('已完成三分之一')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a_scan1 = 0\n",
    "X_a_scan = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成三分之二\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Layers.keys())-1):\n",
    "    X_a_scan2 = Layers[i].push_thru_data(X_a_scan2)\n",
    "last = len(Layers.keys())-1\n",
    "y_pred2 = Layers[last].predict(X_a_scan2)\n",
    "X_a_scan2 = 0\n",
    "print('已完成三分之二')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成预测\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Layers.keys())-1):\n",
    "    X_a_scan3 = Layers[i].push_thru_data(X_a_scan3)\n",
    "last = len(Layers.keys())-1\n",
    "y_pred3 = Layers[last].predict(X_a_scan3)\n",
    "X_a_scan3 = 0\n",
    "print('完成预测')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate((y_pred1,y_pred2,y_pred3),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,:)\n",
      "(100,:)\n",
      "(200,:)\n",
      "(300,:)\n",
      "(400,:)\n",
      "(500,:)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_predict = np.array(y_pred).reshape(row_side,loc_side)\n",
    "img = np.zeros([row_side,loc_side,3])\n",
    "for i in range(class_predict.shape[0]):\n",
    "        for j in range(class_predict.shape[1]):\n",
    "                if class_predict[i][j]  == 0 :  # 背景\n",
    "                        img[i][j][0] = 0\n",
    "                        img[i][j][1] = 0\n",
    "                        img[i][j][2] = 0\n",
    "                elif class_predict[i][j] == 1: # 沙漠\n",
    "                        img[i][j][0] = 0\n",
    "                        img[i][j][1] = 255\n",
    "                        img[i][j][2] = 255\n",
    "                elif class_predict[i][j] == 2: # 戈壁\n",
    "                        img[i][j][0] = 190\n",
    "                        img[i][j][1] = 190\n",
    "                        img[i][j][2] = 190\n",
    "                elif class_predict[i][j] == 3: # 绿洲\n",
    "                        img[i][j][0] = 0\n",
    "                        img[i][j][1] = 255\n",
    "                        img[i][j][2] = 0\n",
    "                elif class_predict[i][j] == 4: # 水系\n",
    "                        img[i][j][0] = 255\n",
    "                        img[i][j][1] = 250\n",
    "                        img[i][j][2] = 87\n",
    "\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\"({0},:)\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cv.imwrite(savepath,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
